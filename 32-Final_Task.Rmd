  ---
title: "Final Project"
output: html_document
---

# Final Task

This document provides the instructions for the final project that students must present for this class.

In short

In this project, you must provide:
• your best vector of predictions of length 140 x 7 = 980, necessarily called our.predictions,
• a fully reproducible Rmd file giving and explaining all the steps and R code used to obtain that vector.

A train.data is provided for you to estimate and select relevant models. There are 7 different responses to be estimated. Models may (should) vary for each response.
Based on these models, the predictions are made on test.data for which you have the predictors but not the responses.

Your vector of predictions will be evaluated with the root mean square error criterion both for the overall performance and for each individual response.

## Data

The file project_data.Rdata contains two data frames, train.data and test.data. These names are self-explanatory. Their dimensions are the following:

```{r
dim(train.data)
## [1] 200 18
dim(test.data)
## [1] 140 11
```

The data used in this problem comes from a study on water quality. Samples were taken from sites on different rivers where the quantities of eight chemical substances were recorded: the maximum pH value (mxPH), the minimum oxygen value (mnO2) as well as the mean values of chloride (Cl), nitrates (NO3), ammonium (NH4), orthophosphate (oPO4), phosphate (PO4) and chlorophyll (Chla). Further information for each observation includes three categorical variables: the season when the sample was taken (season), the river size (size) and the flow velocity (speed). In total, that are 11 predictors for the frequency of 7 plants. In the train.data, these plants are note a1 to a7.
The test.data has the same structure but does not contain the frequencies for each of the 7 plants. Your goal is precisely to estimate them for the 140 observations. Data summaries and visualizations can help you gain insights into the data.

## Some specifics

### Models and their selection

Because there are 7 responses to estimate, you can/should use 7 models. Available techniques include linear models, trees, random forests, etc. . . For every model that you try, provide a short description. Of course, you do not need to describe sub-versions such as yet another polynomial. However, you cannot simply present only your chosen model. The process of selection must also be described. This include parameter selection such as the degree of
polynomial or the size of the tree. Importantly, it also includes methods to evaluate the models (crossvalidation). Your 980 predictions must be stacked into a vector so that they can be compared with the actual values and evaluated with the root mean square error.
Your scored on the quality of the predictions will be determined by comparing your RMSE with mine.

### NA’s

Both data frames contain NA’s. To deal with them, there are various options:
• remove the observations with NA’s,
• fill the blanks,
• use a mix of these two.

Of course, whatever method is chosen in the train data, must similarly be used in the test data. Recall, however, that the test data must never be used in the estimation process.
In this particular case, it is acceptable to delete train observations that seem too noisy to be useful in the estimation.

train.data %>% is.na %>% rowSums %>% table

>> .
>> 0 1 2 6
>> 184 7 7 2

As for the missing values in some variables, you can fill them with an appropriate choice. Here, you have many options. These range from the simplest (and least accurate) such as replacing with the mean of the variable, to the most sophisticated such as replacing each missing value with the prediction of a complex model. Intermediate solutions include exploiting correlations between variables. No matter what option you take, you must be clear about your choice.

test.data %>% complete.cases %>% table

>> .
>> FALSE TRUE
>> 18 122

Recall that the test data also contains missing values. Hence, your predictions will start by filling them by using the same approach that you used with the train data.

## Solution to the final task 

This part of the book focuses on the final task of our group project in Data Science at Hochschule Fresenius (the final task is described in the previous chapter). 

To understand the process of finding solution, we will divide this chapter into two parts:

1. Trial and error 

2. Model used for the solution of the final task

The first part ("Trial and error") focuses on the first steps of finding a solution for the final task, which was the attempt to use multiple linear regressions, in order to analyze the data and find the "best" predictions. It is therefore the first "failure" of finding a solution.

The second part ("Model used for the solution of the final task") focuses on a multiple linear regression model to cross-validation we have found and used as a solution to the problem. This model is therefore our solution to the problem, which is relevant for the project.

## Trial and error 

### First attempt

Our first attempt was to make use of the interaction terms of linear regressions, since interactions can be done between quantitative and categorical variables. We therefore thought it would be very easy to interpret and even visualize, despite the multiple variables given in the task.

Here the example:

Before starting, let's load the required data:

```{r}
load("project_data (1).Rdata")
```

Step 1: Define the data set (Use train.data).

```{r}
train.dt <- lm(a1 ~ mxPH + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data)
```

```{r}
summary(train.dt)
```

Step 2: Get rid of the NA's.

```{r}
is.na(train.data)
na.omit(train.data)
```

```{r}
require(dplyr)
na.omit(train.data) %>% {cor(.[,c("mxPH", "Cl", "NO3", "NH4", "oPO4", "PO4", "Chla")])} %>% round(2)
```


Step 3: Plot the first part dataset given (Data = train.data), which focuses on a1.

```{r}
train.int1 <- lm(a1 ~ mxPH + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data)
summary(train.int1) 
plot(train.int1)
```

Step 4: Plot the remaining parts of the dataset, which focuses on a2-a6.

```{r}
pred1 <- predict(train.int1)

plot(train.data$mxPH, train.data$a1)


t.data <- train.data
t.data$mxPH <- "Yes"

n.data <- train.data
n.data$mxPH <- "No"

pred1.1 <- predict(train.int1, data = t.data)

pred1.2 <- predict(train.int1, data = n.data)

plot(train.data$mxPH, train.data$a1)
lines(train.data$mxPH, train.data$a1, col="red")
lines(train.data$mxPH, train.data$a1, col="black")
train.data
```

This was the final step of our first attempt to find the solution to the problem, yet we didn't know how to proceed from here.

### Second attempt

Our second attempt was to make use of ploynomials with regards to linear regressions. We started by defining the dataset and then tried to find the right degree n. This also didn't work, which is why we made use of other models, in order to find the solution to the problem.

## Model used for the solution of the final task (#THE SOLUTION)

As mentioned before, in order to find the solution to the final task of our project, we decided to make use of a multiple linear regression model to cross validation.

### Solution: Multiple linear regression model to cross validation

Step 1: The first step of using the model is to load the two datasets given, as well as the liabraries needed (train.data & test.data).

```{r}
library(tidyverse)
library(caret)
library(dplyr)
train.data
test.data
```

Step 2: The second step is to handle the NAs by filling blanks with column mean (for train.data & test.data).

```{r}
train.data <- train.data %>% mutate_all(~ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x))
train.data$season <- as.factor(train.data$season)
train.data$size <- as.factor(train.data$size)
train.data$speed <- as.factor(train.data$speed)
```

```{r}
test.data <- test.data %>% mutate_all(~ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x))
test.data$season <- as.factor(test.data$season)
test.data$size <- as.factor(test.data$size)
test.data$speed <- as.factor(test.data$speed)
```

Step 3: The third step of using the model is define in total 7 datasets for the AV's we have and therefore prepare the predictions. This also includes the calculation of the predicted values and the calculation of the RMSE.

1. Multiple linear regression model #1 

```{r}
modela1 <- lm(a1 ~ season + size + speed + mxPH + mnO2 + log(Cl) + NO3 + NH4 + log(oPO4) + log(PO4) + log(Chla), data = train.data)
summary(modela1)
```

>> Calculate predicted values

```{r}
preda1 <- predict.lm(modela1, test.data, na.action = na.pass)
```

>> Calculate RMSE

```{r}
RMSE <- function(pred, obs, dv){
  sqrt(mean((pred-obs)^2, na.rm=T))/sd(dv)
}

RMSEa1 <- RMSE(preda1, train.data$a1, train.data$a1)
```

2. Multiple linear regression model #2

```{r}
modela2 <- lm(a2 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data)
summary(modela2)
```

>> Calculate predicted values

```{r}
preda2 <- predict.lm(modela2, test.data, na.action = na.pass)
```

>> Calculate RMSE

```{r}
RMSEa2 <- RMSE(preda2, train.data$a2, train.data$a2)
```

3. Multiple linear regression model #3

```{r}
modela3 <- lm(a3 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data)
summary(modela3)
```

>> Calculate predicted values

```{r}
preda3 <- predict.lm(modela3, test.data, na.action = na.pass)
```

>> Calculate RMSE

```{r}
RMSEa3 <- RMSE(preda3, train.data$a3, train.data$a3)
```

4. Multiple linear regression model #4

```{r}
modela4 <- lm(a4 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data)
summary(modela4)
```

>> Calculate predicted values

```{r}
preda4 <- predict.lm(modela4, test.data, na.action = na.pass)
```

>> Calculate RMSE

```{r}
RMSEa4 <- RMSE(preda4, train.data$a4, train.data$a4)
```

5. Multiple linear regression model #5

```{r}
modela5 <- lm(a5 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data)
summary(modela5)
```

>> Calculate predicted values

```{r}
preda5 <- predict.lm(modela5, test.data, na.action = na.pass)
```

>> Calculate RMSE

```{r}
RMSEa5 <- RMSE(preda5, train.data$a5, train.data$a5)
```

6. Multiple linear regression model #6

```{r}
modela6 <- lm(a6 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data)
summary(modela6)
```

>> Calculate predicted values

```{r}
preda6 <- predict.lm(modela6, test.data, na.action = na.pass)
```

>> Calculate RMSE

```{r}
RMSEa6 <- RMSE(preda6, train.data$a6, train.data$a6)
```

7. Multiple linear regression model #7

```{r}
modela7 <- lm(a7 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla, data = train.data)
summary(modela7)
```

>> Calculate predicted values

```{r}
preda7 <- predict.lm(modela7, test.data, na.action = na.pass)
```

>> Calculate RMSE

```{r}
RMSEa7 <- RMSE(preda7, train.data$a7, train.data$a7)
```

Step 3: Create table and find the solution.

```{r}
RMSE.table <- matrix(c(RMSEa1, RMSEa2, RMSEa3, RMSEa4, RMSEa5, RMSEa6, RMSEa7), ncol=1, byrow=TRUE)
colnames(RMSE.table) <- c("RMSE")
rownames(RMSE.table) <- c("Model a1:", "Model a2:", "Model a3:", "Model a4:", "Model a5:", "Model a6:", "Model a7:")
RMSE.table <- as.table(RMSE.table)
RMSE.table
```

Our best vector of predictions, called `our.predictions`:

```{r}
our.predictions <- c(preda1, preda2, preda3, preda4, preda5, preda6, preda7)
our.predictions
```

Desciption of the solution for RMSE's:

Since the test.data has the same structure as the train.data but does not contain the frequencies for each of the 7 plants, our goal is precisely to estimate them for the 140 observations.

Within this solution, the lowest test sample RMSE is the preferred model. It produces the best `model fit`.

In this case, the best fit is therefore Model a7.