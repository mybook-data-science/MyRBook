# Overview {#overview}

This chapter aims at providing an overview of some of the main issues addressed in a data science project. Because time is limited, however, these notes will only cover a selected set of topics.  
Actually, this constraint makes this overview even more important because it builds the framework where to place the future techniques learned in or outside this course.

## Universal scope

Data science addresses a very large set of issues. This latter has been expanding in the last decades thanks to the availability of computing power, data sets, new software and theoretical developments.  
Here is a very short list of cases handled by data science (sources: @esl, @isl and @isln).

## Statistical learning

The original postulate is that there exist a relationship between a _response_ \(Y\) variable and, **jointly** a set \(X\) of variables ( _independent variables_, _predictors_, _explanatory variables_).

```{r, out.width = "100%", message = FALSE, warning = FALSE, include=is_latex_output()}
require(gridExtra)
advertising <- read_csv("Advertising.csv") %>%
	select(-X1) 
advertising

p1 <- advertising %>% ggplot(
	mapping= aes(x=TV, y= sales)
	) +
	geom_point() +
	geom_smooth(method='lm', se = FALSE)

p2 <- advertising %>% ggplot(
	mapping= aes(x=radio, y= sales)
	) +
	geom_point() +
  geom_smooth(method='lm', se = FALSE)

p3 <- advertising %>% ggplot(
	mapping= aes(x=newspaper, y= sales)
	) +
	geom_point() +
  geom_smooth(method='lm', se = FALSE)

grid.arrange(p1, p2, p3, ncol=3)

# require(GGally)
# ggpairs(advertising)
```


Then, the general form of the relationship between these variables is as follows.
\[Y=f(X) + \varepsilon \]
where \(\varepsilon\) captures various sources of error.  
We will denote by \(n\) the number of observations, i.e., the number of tuples containing a value of response and a value for each predictor. Also, \(p\) is the number of predictors.   
It is useful to see the different objects of the equation above.
\[\left( \begin{array}{l} y_1 \\ y_2 \\ \vdots \\ y_n \end{array}\right) =f\left( \begin{array}{llll} x_{11} & x_{12} & \dots & x_{1p} \\
x_{21} & x_{22} & \dots & x_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \dots & x_{np} \end{array}\right) 
+
\left( \begin{array}{l} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{array}\right)
\] 


The **goal of statistical learning is to estimate** (learn, determine, guess,...)  \(f()\).  
Figure \@ref(fig:plot-simf12-book) illustrates the data learning process by plotting \(Y\) for the values of \(X\), a unique vector (left) along with the errors measured as the difference between the observations and the true function (right). Notice that the true function is known in this case because the data is simulated.  
The different techniques explored here are designed to come as close as possible to the true, blue line.  

## Use of statistical learning

There are two main reasons one would want to estimate \(f()\).  

### Prediction

In many occasions, the independent variables are known but the response is not. Therefore, \(f()\) can be used to _predict_ these values. These predictions are noted by
\[\hat{Y}=\hat{f}(X)\]
where \(\hat{f}\) is the estimated function for \(f()\).

### Inference

The estimated \(\hat{f}()\) is also used to answer questions about the relationship between the independent variables and the response variables, such as:

  - which predictors contributes the response,
  - how much each predictor contributes to the response,
  - what is the form of the relationship.

This use of statistical learning is not the main interest of these notes (see Section \@ref(aiwhy)).     

## Classification setting

In classification problems, one cannot calculate the MSE. Instead, the common measure to assess accuracy if the **error rate**, the proportion or miss-classified observations, defined in the training set as
\[\frac{1}{n}\sum_i^n I\big(y_i\neq\hat{y}_i\big)\]
Similarly, in the test data, we can calculate
\[\text{Ave}\big(I(y_i\neq\hat{y}_i)\big)\] 


### Bayes classifier

It can be shown that the error rate in the test data is minimized by a classifier that assigns the observation to the class  for which it has the highest probability of belonging.  
This classifier is called _Bayes classifier_ and is based on the conditional probabilities for each \(j\) 
\[Pr(Y=j \vert X=x_0)\]
The problem is that, unless the data is simulated (as below), these probabilities are not known.
Figure \@ref(fig:plot-bayesc01) illustrates a simulated case and draws the contours given by the Bayes classifier. 

### K-nearest neighbors 

As a feasible solution, one could try to estimate the conditional probabilities. Some techniques attempt precisely that.  
Here, we quickly introduce a very simple non-parametric method, _K-nearest neighbors_. As it names indicates, the probability of a class is estimaded by an averaging of the \(K\) closest observations. Formely

\[Pr(Y=j \vert X=x_0)= \frac{1}{K}\sum_{i \in N(x_0)}I(y_i=j) \]








