---
title: "Linear Regression"
output: html_document
---

# Linear Regression

Let's start with a general defintion of what linear regression is and what it is used for:

In general, linear regression is a linear approach to modeling the relationship between a scalar response and one or more explanatory variables. In other words, linear regression is a useful tool to predict the value of an outcome variable Y based on one or more input predictor variables X. 

Objective: The objective is therefore to establish a linear relationship (mathematical formula) between the predictor variable(s) and the response variable, in order to be able to use this formula to estimate the value of the response variable Y, when only the predictor (Xs) variables or values are known.

The above mentioned mathematical equation can be generalized as follows:

\[Y=\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p + \varepsilon\]

β1 is the intercept
β2 is the slope

Collectively, they are called regression coefficients. 

ϵ is the error term (also descriped as the part of Y the regression model is unable to explain)

Example Problem:

For this analysis, we will use the cars dataset that comes with R by default. cars is a standard built-in dataset, that makes it convenient to demonstrate linear regression in a simple and easy to understand fashion. You can access this dataset simply by typing in cars in your R console. You will find that it consists of 50 observations(rows) and 2 variables (columns) – dist and speed. Lets print out the first six observations here..


```{r, warning = FALSE, message = FALSE}
require(tidyverse)
advertising <- read_csv("Advertising.csv")
advertising

advertising <- read_csv("Advertising.csv")
advertising
```

Before we begin building the regression model, it is a good practice to analyze and understand the variables. The graphical analysis and correlation study below will help with this.

## Graphical Analysis

The aim of this exercise is to build a simple regression model that we can use to predict Distance (dist) by establishing a statistically significant linear relationship with Speed (speed). But before jumping in to the syntax, lets try to understand these variables graphically. Typically, for each of the independent variables (predictors), the following plots are drawn to visualize the following behavior:

Scatter plot: Visualize the linear relationship between the predictor and response.

```{r}
scatter.smooth(x=advertising$sales, y=advertising$TV, main="Sales ~ TV")  # scatterplot
```

Box plot: To spot any outlier observations in the variable. 

Generally, any datapoint that lies outside the 1.5 * interquartile-range (1.5 * IQR) is considered an outlier, where, IQR is calculated as the distance between the 25th percentile and 75th percentile values for that variable.

```{r}
par(mfrow=c(1, 2))  # divide graph area in 2 columns
boxplot(advertising$sales, main="Sales", sub=paste("Outlier rows: ", boxplot.stats(advertising$sales)$out))  # box plot for 'Sales'
boxplot(advertising$TV, main="TV", sub=paste("Outlier rows: ", boxplot.stats(advertising$TV)$out))  # box plot for 'TV'
```

Density plot: To see the distribution of the predictor variable. 

```{r}
library(e1071)
par(mfrow=c(1, 2))  # divide graph area in 2 columns
plot(density(advertising$sales), main="Density Plot: Sales", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(advertising$sales), 2)))  # density plot for 'Sales'
polygon(density(advertising$sales), col="red")
plot(density(advertising$TV), main="Density Plot: TV", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(advertising$TV), 2)))  # density plot for 'TV'
polygon(density(advertising$TV), col="blue")
```

Correlation:

Correlation is a statistical measure that suggests the level of linear dependence between two variables, that occur in pair – just like what we have here in sales and TV. 

High correlation = Correlation close to 1

Inverse relationship = Correlation close to -1

Weak correlation = Correlation close to 0

```{r}
cor(advertising$sales, advertising$TV)  # calculate correlation between sales and TV 
```

## Simple linear regression

After applying a graphical analysis, we will now move over to a simple linear regression and apply the data from above.

This section therefore builds around the example of the simple linear regression of _sales_ on the amount of _TV_ advertising in the _Advertising_ data set.  
To fix ideas, the linear model estimated here is
\[\text{sales} = \beta_0 + \beta_1\times \text{TV}  + \varepsilon\]

The **estimation** of the model is carried with the function `lm` from the built-in `stats` package. The result of the estimation is an object to assigned to a name.

```{r}
estimation.mod <- lm(sales ~ TV, data = advertising)
```

The content of this linear regression object is better described with the function `summary`.

```{r}
summary(estimation.mod)
names(estimation.mod)

estimation.mod$fitted.values

names(summary(estimation.mod))
summary(estimation.mod)$r.squared
summary(estimation.mod)$df



```

```{r}
# tidyverse alternative
advertising %>%
  mutate(y.hat1 = estimation.mod$fitted.values,
         y.hat2 <- predict(estimation.mod),
         y.hat3 <- estimation.mod$coefficients[1] + estimation.mod$coefficients[2]*TV)

advertising
advertising$TV
advertising$y.hat1 <- estimation.mod$fitted.values
advertising$y.hat2 <- predict(estimation.mod)
advertising$y.hat3 <- estimation.mod$coefficients[1] + estimation.mod$coefficients[2]*advertising$TV


advertising

plot(advertising$TV, advertising$sales)
lines(advertising$TV, advertising$y.hat2, col="blue")
lines(advertising$TV, advertising$y.hat1, col="red")
```

Let's now have a look at the **errors/ residuals** of our prediction.
```{r}
advertising$residuals <- advertising$sales - advertising$y.hat2

sum(advertising$residuals)
```

Prediction of sales: We can now use the model to predict for data that not in the training data.

```{r}
# predict sales for TV=400, 500, 600...

# brute force way, very tedious in general
sales_tv_400 <- estimation.mod$coefficients[1] + estimation.mod$coefficients[2]*400
sales_tv_400

# 'predict' way
# step 1: create a data.frame for the new X data
# step 2: predict with newdata= newdata

my.boss.question <- data.frame(TV=c(400, 500, 600))
my.boss.question

sales_tv_boss <- predict(estimation.mod, newdata = my.boss.question ) 
sales_tv_boss

```


This last call alone provides a variety of statistics given in the package of `isln`.
Notice that parts of this regression object can be accessed through sub-setting of the object and used, once we know under what name they are stored, which we obtain by the next call (see `?lm` for the value of the function, i.e., what the function returns).

```{r}
names(estimation.mod)
# estimation.mod %>% names
```

Alternatively, and somehow more surprising, all the numbers given by the `summary` function can also be accessed in the same fashion:

```{r}
estimation.mod %>% summary %>% names
```

As an example, the following table can be built by inline R-code (`isln`), with calls such as `summary(estimation.mod)$fstatistic[1]`, possibly with rounding as well.

| Quantity | Value |
|:--|:--|
| Residual Standard Error | `r summary(estimation.mod)$sigma %>% round(2)` |
| \(R^2\) | `r summary(estimation.mod)$r.squared %>% round(3)` |
| \(F\)-statistic | `r summary(estimation.mod)$fstatistic[1] %>% round(1)` |

Table: Results for simple linear regression (Advertising)


As for the **confidence interval** of \(\beta_1\) (`isln`), i.e., the random interval in which, under repeated sampling, the true parameter would fall \(95\%\) of the time, we type the code below.

```{r}
c.i.beta1 <- c(summary(estimation.mod)$coefficients[2,1] -
                 2 * summary(estimation.mod)$coefficients[2,2],
               summary(estimation.mod)$coefficients[2,1] +
                 2 * summary(estimation.mod)$coefficients[2,2])
c.i.beta1 %>% round(3)
```

One of the main reasons the simple linear regression is exposed is its graphical appeal. In particular, the _ordinary least squares_ criterion can be visualized with a graph of the residuals with respect to the fit.  
This visualization builds on the regression fit which we obtain first below in two alternative ways.  

1. The **fitted line** can be obtained with the fitted values of the model given by the `lm` function, i.e., `.$fitted.values`.

```{r}
tibble(advertising$TV, advertising$sales, estimation.mod$fitted.values)
```

Digging into the details, these fitted values  are simply obtained thanks to the estimated parameters using the \(X\) values (_TV_, in this case). 

```{r}
manually.fitted <- estimation.mod$coefficients[1] + estimation.mod$coefficients[2] * advertising$TV
all.equal(as.vector(estimation.mod$fitted.values), manually.fitted)
```

2. The second approach uses the function `predict` from the built-in `stats` package. The function is a bit versatile as its behavior depends on which type of objects it is fed with.  
Applied to a `lm` object, it will, by default, return **predictions** for each of the \(X\) values  used to fit the model. 

```{r}
all.equal(as.vector(estimation.mod$fitted.values), manually.fitted, predict(estimation.mod))
```

Thanks to the fitted/predicted value, we can calculate the values above about the quality of the fit. Here are a few lines of code to manually calculate these statistics.

```{r}
## R2
TSS <- sum((advertising$sales - mean(advertising$sales))^2)
TSS

RSS <- sum((advertising$sales -  predict(estimation.mod))^2)
RSS

R2 <- 1 - RSS/TSS
R2 %>% round(3)

## RSE
n <- length(advertising$sales)
p <- length(estimation.mod$coefficients) - 1
RSE <- sqrt(RSS /(n - p - 1))
RSE %>% round(2)
# notice that this is more or less the sd of the errors
sd(advertising$sales -  predict(estimation.mod)) %>% round(2)

## F-statistic
F <- (TSS - RSS)/p * (RSS/(n-p-1))^(-1)
F %>% round(1)
```

We can now turn to the **graph** of the fit.  
As much as possible, we want to use `ggplot` for our graphs. In this case, we must first add the predicted/fitted values to the data frame. There are various, though similar ways to achieve that first step, including one with `geom_smooth`.

```{r, out.width = "100%", message = FALSE, warning = FALSE, include=TRUE}

advertising <- advertising %>%
	mutate(fit_TV= estimation.mod$fitted.values)
	# mutate(fit_TV= predict(estimation.mod))
	# mutate(fit_TV = predict(lm(sales ~ TV), interval = "confidence")[,"fit"])
         

p1 <- advertising %>% ggplot(
	mapping= aes(x=TV, y= sales)
	) +
	geom_point(size=1, shape=21) +
	#geom_smooth(method='lm', se = TRUE) + # another alternative for the fit
	geom_line(aes(y=fit_TV), color ="blue", size =1) +
	geom_segment(aes(x = TV, y = sales, xend = TV, yend = fit_TV, colour = "red")) +
	theme(legend.position = "none")
p1
```

## Multiple linear regression

The treatment of the multiple linear regression is similar to the part above. The differences include:

* The command for the `lm` function;
* No graphical representation;
* The often tedious interpretation of the coefficients.

We proceed by estimating 

\[\text{sales} = \beta_0 + \beta_1\times \text{TV} + \beta_2\times \text{radio} + \beta_3\times \text{newspaper}  + \varepsilon\]


```{r}
estimation.mod2 <- lm(sales ~ TV + radio + newspaper, data = advertising)
```

Importantly, the `+` sign does not mean that the regression is on the sum of the variables. 

Instead, the expression should be read "regression of sales on TV _plus on_ radio _plus on_ newspaper".

```{r}
summary(estimation.mod2)
```

For the interpretation of the coefficients, the correlations between the predictors is often useful.

```{r}
advertising %>% {cor(.[,c("TV", "radio", "newspaper")])} %>% round(4)
```

## Categorical regressors

The predictors of the model need not be numeric variables. They can also be factors.

In order to closely follow `isln`, we now load another data set, `Credit` from the package `ISLR`.

```{r}
require(ISLR)
data("Credit")
str(Credit)
```

The scatter plots for each pair of variables is a useful visualization.

```{r, out.width = "100%", message = FALSE, warning = FALSE, include=TRUE}
require(GGally)
ggpairs(Credit[,c("Balance", "Age", "Cards", "Education",
                  "Income", "Limit", "Rating")])
```

The key element in this section is the qualitatitive / categorical predictors, also called factor variables.   
In the `Credit` data set, variables such `gender` or `student` are factors.  

We illustrate here how these variables can be used in a linear regression.

```{r}
model.credit <- lm(Balance ~ Gender, data = Credit)
summary(model.credit)
```

This seems to work seemlessly. There is a detail, however that must be brought into the light.  
`R` has automatically created a dummy variable. This may or may not be the intended choice.  
The exact choice can be examined with the following call.

```{r}
contrasts(Credit$Gender)
```
We see that the variable `GenderFemale` (read in the upper part of the table) shown in the summary of the model takes the value \(0\) if the individual is Male and \(1\) if the individual is Female.   
With multiple factors in the variable, the reading of the table must be well understood.

```{r}
contrasts(Credit$Ethnicity)
```

These values are used in the next model.

```{r}
model.credit2 <- lm(Balance ~ Ethnicity, data = Credit)
summary(model.credit2)
```

Notice that these dummy values are created in alphabetical order. Hence, the first will always server as reference.  
This behavior can be changed thanks to the `relevel` function.

```{r}
Credit$Ethnicity <- relevel(Credit$Ethnicity, ref = "Caucasian")
contrasts(Credit$Ethnicity)
```


## Interactions terms

Notice that the \(\beta\)'s represent the average effect of a one unit change in the predictor on the response.  
The assumption of a constant effect on the response, i.e., constant \(\beta_i\), is often difficult to sustain. For instance, in case of synergies of the advertising media, the effect of one particular media depends on how much of the other media are already been run.  
Interactions terms constitute a variation of the linear regression whose aim is precisely to allow for non-constant effects of variables on the response.  
The interaction between variables are built with the `:` symbol. For instance, the result in @isln, Chap. 3, slide 37 is obtained through the following call.

```{r}
model.interaction1 <- lm(sales ~ TV + radio + TV:radio, data = advertising)
summary(model.interaction1) 

## alternatively, use the cross *
lm(sales ~ TV*radio, data = advertising)
```

Interactions can be done between quantitative and categorical variables. This case is actually the very easy to interpret and even visualize, despite the multiple variables.


```{r}
model.interaction2 <- lm(Balance ~  Income + Student, data = Credit)
summary(model.interaction2) 
model.interaction3 <- lm(Balance ~  Income + Student + Income:Student, data = Credit)
summary(model.interaction3) 


y.hat4 <- predict(model.interaction2)

plot(Credit$Income, Credit$Balance)
lines(Credit$Income, y.hat4, col="red")

s.data <- Credit
s.data$Student <- "Yes"

n.data <- Credit
n.data$Student <- "No"

y.hat5 <- predict(model.interaction2, newdata = s.data)

y.hat6 <- predict(model.interaction2, newdata = n.data)

plot(Credit$Income, Credit$Balance)
lines(Credit$Income, y.hat5, col="red")
lines(Credit$Income, y.hat6, col="black")
Credit
```

We can plot these different models.

```{r, out.width = "100%", message = FALSE, warning = FALSE, include=TRUE, fig.cap= "Fits of models without (left) and with (right) interactions terms of Income and Student for Students (red) and not Students (black)."}
s.data <- Credit
s.data$Student <- "Yes"

ns.data <- Credit
ns.data$Student <- "No"

cols <- c("Student"="red", "Yes" ="red", "Not student"="black", "No"="black")
p1 <- Credit %>%
	mutate(fit.student = predict(model.interaction2, newdata = s.data),
		fit.not.student = predict(model.interaction2, newdata = ns.data)) %>%
	ggplot(aes(x=Income, y=Balance)) +
  geom_point(aes(x=Income, y=Balance, color=Student)) +
  geom_line(aes(y=fit.student, color="Student")) +
  geom_line(aes(y=fit.not.student, color="Not student")) +
  scale_colour_manual(values=cols) +
  theme(legend.position = "none")

p2 <- Credit %>%
	mutate(fit.student = predict(model.interaction3, newdata = s.data),
	       fit.not.student = predict(model.interaction3, newdata = ns.data)) %>%
  ggplot(aes(x=Income, y=Balance)) +
  geom_point(aes(x=Income, y=Balance, color=Student)) +
  geom_line(aes(y=fit.student, color="Student")) +
  geom_line(aes(y=fit.not.student, color="Not student")) +
  scale_colour_manual(values=cols) +
  theme(legend.position = c(50, 1000),
          legend.direction = "horizontal")
require(gridExtra)
grid.arrange(p1, p2, ncol=2)
```

## Polynomials of degree n {#polyn}

Another very useful extension of the linear model is to include powers of variables in order to capture non-linear effects. This seems to be a contradiction in terms, but a possible answer could be that the model is still linear in the coefficients.  

To fix ideas, here is an example of fitting a quadratic model.

\[\text{mpg} = \beta_0 + \beta_{1}\times \text{horsepower} + \beta_{2}\times \text{horsepower}^2 + \varepsilon\]

This model can be estimated in the `Auto` data set of the `ISLR` package.

```{r}
require(ISLR)
data("Auto")
model.pd1 <- lm(mpg ~ horsepower, data = Auto)
summary(model.pd1)
model.pd2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)
summary(model.pd2) 
```
Notice the use of the `I` function which, in a formula, inhibits the interpretation of operators such as `+` and `^` as formula operators but, instead, makes them be used as arithmetical operators.  

For higher degrees of polynomial, it can become cumbersome to write all the degrees. That is where the function `poly` is handy.

```{r}
require(ISLR)
data("Auto")
model.pd5 <- lm(mpg ~ poly(horsepower, 5), data = Auto)
model.pd9 <- lm(mpg ~ poly(horsepower, 9), data = Auto)

```

Again, the advantage of the linear regression with a single predictor is the visualization of its fits, as illustrated below.

```{r, out.width = "100%", message = FALSE, warning = FALSE, include=TRUE, fig.cap= "Fits of mpg for various degrees of the polynomial of horsepower."}
Auto <- Auto %>%
	mutate(fit1 = predict(model.pd1),
	fit2 = predict(model.pd2),
	fit5 = predict(model.pd5),
	fit9 = predict(model.pd9))

cols <- c("Deg.1", "Deg.2", "Deg.5", "Deg.9")
Auto %>% 
	ggplot(aes(x=horsepower, y=mpg)) +
	geom_point() +
	geom_line(aes(y=fit1, color="Deg.1"), size =2) +
	geom_line(aes(y=fit2, color="Deg.2"), size =2) +
	geom_line(aes(y=fit5, color="Deg.5"), size =2) +
  geom_line(aes(y=fit9, color="Deg.9"), size =2) +
	theme(legend.title = element_blank(), 
		legend.position = "bottom", 
		legend.direction = "horizontal")

```


```{r}
fita1 <- lm(sales ~ TV + radio + newspaper, data=advertising )
summary(fita1)

fita2 <- lm(sales ~ poly(TV,5) + radio + newspaper , data=advertising )
summary(fita2)

fita3 <- lm(sales ~ poly(TV,5) + poly(radio,3) + poly(newspaper,6) , data=advertising )
summary(fita3)

fita4 <- lm(sales ~ TV + radio + newspaper + TV:radio, data=advertising )
summary(fita4)

fita5 <- lm(sales ~ TV*radio*newspaper, data=advertising )
summary(fita5)

fita6 <- lm(sales ~ poly(TV,2)*poly(radio,2)*poly(newspaper,2), data=advertising )
summary(fita6)

length(fita6$coefficients)
```


