# Trees

## Introduction

Recursive partitioning is a fundamental tool in data mining. It helps us explore the stucture of a set of data, while developing easy to visualize decision rules for predicting a categorical (classification tree) or continuous (regression tree) outcome. This section briefly describes CART modeling, conditional inference trees, and random forests.

Regression and classification trees can be generated through the rpart package.

```{r}
#load packages

require(data.tree)
require(randomForest)
require(rpart)

require(ISLR)
require(MASS)
require(tidyverse)

```

## Regression trees
Let`s look for a baseball salary example to illustrate data with a graph:

```{r}
require(ggplot2)
data("Hitters")

hist(Hitters$Salary)

Hitters$Salary <- log(Hitters$Salary)
hist(Hitters$Salary)

Hitters %>%
  mutate(Salary = log(Salary))

hist(Hitters$Salary)

plot(Hitters$Years, Hitters$Hits, col = Hitters$Salary)


#another way

Hitters  %>%
  ggplot(aes(x=Years, y=Hits, col=Salary)) +
  geom_point()
#The salary level is shown by colour/shape from low (darker blue) to high (lighter blue)

```



A tree for this data (log) `Salary`... depending on `Years` and `Hits`:


```{r}
b.tree <- rpart(Salary ~ Years + Hits, data = Hitters)
plot(b.tree)
text(b.tree, pretty = 0)

plotcp(b.tree)

min.of.cp <- b.tree$cptable[which.min(b.tree$cptable[, "xerror"]), "CP"]

prune.b.tree <- prune(b.tree, cp = 0,071)
plot(prune.b.tree)
text(prune.b.tree, pretty = 0)

```
Decision treers are typically are drawn upside down, because than the leaves are at the bottom of the tree.
The points along the tree where the predictor space is split are referred to as internal nodes. 
In the example hitters tree, the two internal nodes are indicated by the text Years < 3.5 and Hits < 117.5.

## Classification trees

The classification trees are very similar to the regression trees. But there is a difference in the purpose for. 
classification trees are used to predict that every observation belongs to the most commonly occuring class of training obervations in the region to which it belongs. Recursive binary splitting is used to grow a classification tree.

Here an example of a classification tree with another data set:

```{r}
library(rpart)
data("kyphosis")

#grow tree
fit <- rpart(Kyphosis ~ Age + Number + Start,
   method="class", data=kyphosis)

printcp(fit) # display the results 
plotcp(fit) # visualize cross-validation results 
summary(fit) # detailed summary of splits

#plot tree
plot(fit, uniform=TRUE, 
   main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)

#prune the tree
pfit<- prune(fit, cp=   fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
```

>> Command, if you want to plot the pruned tree:

```{r
plot(pfit, uniform=TRUE, 
   main="Pruned Classification Tree for Kyphosis")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
```

## Avoiding over-fitting of trees with Bagging

Bagging runs with package `randomForest` with the reason for reducing variance of statistical methods.

```{r, message = FALSE, warning = FALSE, eval=FALSE, error=TRUE}
library(randomForest)
data_frame(Boston)
boston_bag = randomForest(medv ~ ., data = Boston, mtry = 18, #mtry is number of predictors
                          importance = TRUE, ntrees = 500)
boston_bag
```


## Random Forest

Random forests improve predictive accuracy by generating a large number of bootstrapped trees (based on random samples of variables), classifying a case using each tree in this new "forest", and deciding a final predicted outcome by combining the results across all of the trees (an average in regression, a majority vote in classification). Breiman and Cutler's random forest approach is implimented via the randomForest package.

Here is an example:

```{r}
p <- length(names(Hitters)) -1

Hitters <- Hitters %>%
  na.omit(Hitters)
hit.bag <- randomForest(Salary ~ ., data = Hitters, mtry = p , importance = TRUE, ntrees = 500)
hit.bag

hit.rf <- randomForest(Salary ~ ., data = Hitters, mtry = round(sqrt(p),0) , importance = TRUE, ntrees = 1000)
hit.rf

hit.rf2 <- randomForest(Salary ~ ., data = Hitters, mtry = round(p/2,0) , importance = TRUE, ntrees = 1000)
hit.rf


#what`s the best?
hit.bag
hit.rf2
hit.rf

train.index <- sample(1:nrow(Hitters), ceiling(nrow(Hitters)/2))

thit.bag <- randomForest(Salary ~ ., data = Hitters[train.index,], mtry = p , importance = TRUE, ntrees = 500)
hit.bag

thit.rf <- randomForest(Salary ~ ., data = Hitters[train.index,], mtry = round(sqrt(p),0) , importance = TRUE, ntrees = 1000)
hit.rf

thit.rf2 <- randomForest(Salary ~ ., data = Hitters[train.index,], mtry = round(p/2,0) , importance = TRUE, ntrees = 1000)
hit.rf

pre.thit.bag <- predict(thit.bag, newdata = Hitters[-train.index,])

pre.thit.rf <- predict(thit.rf, newdata = Hitters[-train.index,])

pre.thit.rf2 <- predict(thit.rf2, newdata = Hitters[-train.index,])

sqrt(mean( (pre.thit.bag- Hitters[-train.index, "Salary"])^2, na.rm = TRUE))


```

Another short example of random forest with our data set from classification trees:

```{r}
library(randomForest)
fit <- randomForest(Kyphosis ~ Age + Number + Start,   data=kyphosis)
print(fit) # view results 
importance(fit) # importance of each predictor

```